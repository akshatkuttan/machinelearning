{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87ee50c",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Lab 8**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396a1a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T17:39:23.382396800Z",
     "start_time": "2024-04-08T17:39:07.239612800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkmeg\\AppData\\Local\\Temp\\ipykernel_5192\\1912598261.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root feature index: 5\n",
      "Root feature: spectral_centroid_mean\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class DecisionTreeRootFinder:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def calculate_entropy(self, y):\n",
    "        # Calculate entropy of a given set of labels\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        entropy = 0\n",
    "        total_samples = len(y)\n",
    "        for count in class_counts:\n",
    "            p = count / total_samples\n",
    "            entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def calculate_information_gain(self, feature_column):\n",
    "        # Calculate information gain for a given feature column\n",
    "        total_entropy = self.calculate_entropy(self.y)\n",
    "        unique_values, value_counts = np.unique(feature_column, return_counts=True)\n",
    "        weighted_entropy = 0\n",
    "        total_samples = len(self.y)\n",
    "        for value, count in zip(unique_values, value_counts):\n",
    "            subset_y = self.y[feature_column == value]\n",
    "            weighted_entropy += (count / total_samples) * self.calculate_entropy(subset_y)\n",
    "        information_gain = total_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def find_root_feature(self):\n",
    "        # Find the root feature with the highest information gain\n",
    "        num_features = self.X.shape[1]\n",
    "        best_feature = None\n",
    "        max_information_gain = -np.inf\n",
    "        for i in range(1, num_features):  # Start loop from index 1\n",
    "            information_gain = self.calculate_information_gain(self.X[:, i])\n",
    "            if information_gain > max_information_gain:\n",
    "                max_information_gain = information_gain\n",
    "                best_feature = i\n",
    "        return best_feature\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('features_3_sec.csv')\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "target_column = 'label'\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(['filename', target_column], axis=1)\n",
    "y = df[target_column]\n",
    "\n",
    "# Create an instance of DecisionTreeRootFinder\n",
    "root_finder = DecisionTreeRootFinder(X.values, y.values)\n",
    "\n",
    "# Find the root feature index\n",
    "root_feature_index = root_finder.find_root_feature()\n",
    "\n",
    "# Print the root feature index and its corresponding feature name\n",
    "print(\"Root feature index:\", root_feature_index)\n",
    "print(\"Root feature:\", X.columns[root_feature_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd791bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T17:40:22.052379600Z",
     "start_time": "2024-04-08T17:40:21.821417400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root feature index: 7\n",
      "Root feature: spectral_bandwidth_mean\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeRootFinder:\n",
    "    def __init__(self, X=None, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def calculate_entropy(self, y):\n",
    "        # Calculate entropy of a set of labels\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        entropy = 0\n",
    "        total_samples = len(y)\n",
    "        for count in class_counts:\n",
    "            p = count / total_samples\n",
    "            entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def calculate_information_gain(self, feature_column):\n",
    "        # Calculate information gain for a given feature column\n",
    "        total_entropy = self.calculate_entropy(self.y)\n",
    "        unique_values, value_counts = np.unique(feature_column, return_counts=True)\n",
    "        weighted_entropy = 0\n",
    "        total_samples = len(self.y)\n",
    "        for value, count in zip(unique_values, value_counts):\n",
    "            subset_y = self.y[feature_column == value]\n",
    "            weighted_entropy += (count / total_samples) * self.calculate_entropy(subset_y)\n",
    "        information_gain = total_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def find_root_feature(self):\n",
    "        # Find the root feature with the highest information gain\n",
    "        num_features = self.X.shape[1]\n",
    "        best_feature = None\n",
    "        max_information_gain = -np.inf\n",
    "        for i in range(num_features):\n",
    "            information_gain = self.calculate_information_gain(self.X[:, i])\n",
    "            if information_gain > max_information_gain:\n",
    "                max_information_gain = information_gain\n",
    "                best_feature = i\n",
    "        return best_feature\n",
    "\n",
    "    def bin_continuous_feature(self, feature_column, num_bins=None, binning_type='equal_width'):\n",
    "        # Bins a continuous feature into categorical bins\n",
    "        if num_bins is None:\n",
    "            num_bins = 10  # Default number of bins\n",
    "\n",
    "        if binning_type == 'equal_width':\n",
    "            # Divide the range of feature values into num_bins equal-width intervals\n",
    "            bins = np.linspace(np.min(feature_column), np.max(feature_column), num_bins + 1)\n",
    "            binned_feature = np.digitize(feature_column, bins)\n",
    "\n",
    "        elif binning_type == 'frequency':\n",
    "            # Bin the data based on frequency of occurrence\n",
    "            _, bins = np.histogram(feature_column, bins=num_bins)\n",
    "            binned_feature = np.digitize(feature_column, bins)\n",
    "\n",
    "        return binned_feature\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('features_3_sec.csv')\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "target_column = 'label'\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(['filename', target_column], axis=1)\n",
    "y = df[target_column]\n",
    "\n",
    "# Binning continuous features\n",
    "continuous_features = X.select_dtypes(include=['float', 'int']).columns\n",
    "for feature in continuous_features:\n",
    "    # Instantiate DecisionTreeRootFinder and bin the feature\n",
    "    X[feature] = DecisionTreeRootFinder(X, y).bin_continuous_feature(X[feature], num_bins=5, binning_type='equal_width')\n",
    "\n",
    "# Find the root feature\n",
    "root_finder = DecisionTreeRootFinder(X.values, y.values)\n",
    "root_feature_index = root_finder.find_root_feature()\n",
    "\n",
    "print(\"Root feature index:\", root_feature_index)\n",
    "print(\"Root feature:\", X.columns[root_feature_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e0eb1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:03:22.049031Z",
     "start_time": "2024-04-08T17:52:13.419524100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.464964964964965\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, value=None, left=None, right=None):\n",
    "        \"\"\"\n",
    "        Initialize a node in the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        - feature_index: Index of the feature to split on\n",
    "        - threshold: Threshold value for the feature\n",
    "        - value: Value to return if this is a leaf node\n",
    "        - left: Left child node\n",
    "        - right: Right child node\n",
    "        \"\"\"\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - max_depth: Maximum depth of the decision tree\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def calculate_entropy(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the entropy of a set of labels.\n",
    "\n",
    "        Parameters:\n",
    "        - y: Array of labels\n",
    "\n",
    "        Returns:\n",
    "        - entropy: Entropy of the label distribution\n",
    "        \"\"\"\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        entropy = 0\n",
    "        total_samples = len(y)\n",
    "        for count in class_counts:\n",
    "            p = count / total_samples\n",
    "            entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def calculate_information_gain(self, X, y, feature_index, threshold):\n",
    "        \"\"\"\n",
    "        Calculate the information gain for a given feature split.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix\n",
    "        - y: Array of labels\n",
    "        - feature_index: Index of the feature to split on\n",
    "        - threshold: Threshold value for the feature split\n",
    "\n",
    "        Returns:\n",
    "        - info_gain: Information gain achieved by the feature split\n",
    "        \"\"\"\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        left_y, right_y = y[left_mask], y[right_mask]\n",
    "\n",
    "        entropy_parent = self.calculate_entropy(y)\n",
    "        entropy_left = self.calculate_entropy(left_y)\n",
    "        entropy_right = self.calculate_entropy(right_y)\n",
    "\n",
    "        total_samples = len(y)\n",
    "        info_gain = entropy_parent - (len(left_y) / total_samples * entropy_left + len(right_y) / total_samples * entropy_right)\n",
    "        return info_gain\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best feature split based on maximum information gain.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix\n",
    "        - y: Array of labels\n",
    "\n",
    "        Returns:\n",
    "        - best_feature_index: Index of the best feature to split on\n",
    "        - best_threshold: Threshold value for the best feature split\n",
    "        \"\"\"\n",
    "        num_features = X.shape[1]\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        max_info_gain = -np.inf\n",
    "\n",
    "        for feature_index in range(num_features):\n",
    "            unique_values = np.unique(X[:, feature_index])\n",
    "            for threshold in unique_values:\n",
    "                info_gain = self.calculate_information_gain(X, y, feature_index, threshold)\n",
    "                if info_gain > max_info_gain:\n",
    "                    max_info_gain = info_gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix\n",
    "        - y: Array of labels\n",
    "        - depth: Current depth of the tree\n",
    "\n",
    "        Returns:\n",
    "        - node: Root node of the decision tree\n",
    "        \"\"\"\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            leaf_value = np.argmax(np.bincount(y))\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        best_feature_index, best_threshold = self.find_best_split(X, y)\n",
    "\n",
    "        if best_feature_index is None:\n",
    "            leaf_value = np.argmax(np.bincount(y))\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        left_mask = X[:, best_feature_index] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        left_X, left_y = X[left_mask], y[left_mask]\n",
    "        right_X, right_y = X[right_mask], y[right_mask]\n",
    "\n",
    "        left_subtree = self.build_tree(left_X, left_y, depth + 1)\n",
    "        right_subtree = self.build_tree(right_X, right_y, depth + 1)\n",
    "\n",
    "        return TreeNode(feature_index=best_feature_index, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the decision tree classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix\n",
    "        - y: Array of labels\n",
    "        \"\"\"\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict_instance(self, x, node):\n",
    "        \"\"\"\n",
    "        Predict the label for a single instance.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Feature vector of a single instance\n",
    "        - node: Current node in the decision tree\n",
    "\n",
    "        Returns:\n",
    "        - Prediction: Predicted label for the instance\n",
    "        \"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self.predict_instance(x, node.left)\n",
    "        else:\n",
    "            return self.predict_instance(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels for multiple instances.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix of multiple instances\n",
    "\n",
    "        Returns:\n",
    "        - Predictions: Array of predicted labels\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self.predict_instance(x, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    df = pd.read_csv('features_3_sec.csv')\n",
    "\n",
    "    # Encode the categorical column 'label'\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['label'])\n",
    "    target_column = 'label'\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X = df.drop(['filename', target_column], axis=1)\n",
    "    y = df[target_column] \n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert the target variable to integer type\n",
    "    label_encoder_y = LabelEncoder()\n",
    "    y_train = label_encoder_y.fit_transform(y_train)\n",
    "    y_test = label_encoder_y.transform(y_test)\n",
    "\n",
    "    # Initialize and train the decision tree classifier\n",
    "    tree = MyDecisionTreeClassifier(max_depth=5)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = tree.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705306a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
